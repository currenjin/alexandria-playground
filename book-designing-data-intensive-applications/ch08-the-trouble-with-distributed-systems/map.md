# Ch08 The Trouble with Distributed Systems

## TL;DR (3문장)
- 네트워크는 **손실,지연,중복,재정렬**이 모두 가능하다. 응답이 없다고 해서 미전달이 아님 → **타임아웃,재시도,멱등성**이 기본 전제.
- **시계**는 신뢰하기 어렵다(스큐,점프). 지연/타임아웃은 **monotonic clock**으로, 순서 판단은 **버전/시퀀스**로.
- 프로세스는 **GC,스톨**로 멈출 수 있고, FD(결함 감지기)는 **오탐/지연** 트레이드오프를 가진다. 정족수/리스/펜싱으로 리더십을 안전하게.

## Key Ideas
- **네트워크 현실**: 손실,중복,재정렬 + 큐잉 지연. → **멱등키/시퀀스/중복제거** 설계.
- **타임아웃 설계**: 홉별 예산분배 + `timeout ≈ 2d + r`(왕복/처리 시간 추정) + **모노토닉** 기준.
- **재시도 제어**: 지수 백오프 + 지터 + **Retry Budget(예: ≤5%)**로 증폭 방지. *멱등 작업만* 재시도.
- **Failure Detector(FD)**: 헬스체크/φ-accrual로 **의심 점수화**. 공격적(빠른 차단) ↔ 보수적(오탐↓) 조절.
- **헤지드 요청**: p95/p99에서 *조건부* 세컨드샷로 tail 레이턴시 절단(중복 안전 필수).
- **정족수(Quorum)**: N, R, W 선택으로 최신성↔가용성 조절. **W+R>N**이면 최신 쓰기와 교집합 보장.
- **리스/펜싱 토큰**: 리더/락 소유권을 시간,토큰으로 방어(클럭 스큐 고려).
- **Exactly-once 환상 깨기**: 현실은 at-least-once/at-most-once → **소비자 멱등,재처리**로 보정.
- **프로세스 스톨**: GC/Stop-the-world → **타임아웃/풀,큐 상한**으로 전파 억제, **관측성**으로 감지.

## Trade-offs
| 선택             | 장점               | 단점             | 언제           |
|----------------|------------------|----------------|--------------|
| **짧은 타임아웃**    | Fail-fast, 빠른 전환 | 오탐↑, 재시도 폭증    | 내부 저지연 링크    |
| **긴 타임아웃**     | 오탐↓              | 복구 느림, 자원 홀드   | 외부/불안정 네트워크  |
| **공격적 FD**     | 장애 격리 빠름         | False-suspect↑ | 핵심 경로 SLO 엄격 |
| **보수적 FD**     | 오탐↓              | 장애 인지 늦음       | 배치/비핵심       |
| **헤지 ON(조건부)** | tail 절단          | 부하↑, 중복처리 필요   | p99 문제가 클 때만 |
| **W,R 크게**     | 최신성/안전↑          | 지연/가용성↓        | 강한 정합 필요     |
| **단일 리더 + 리스** | 단순/일관성↑          | 리더 장애시 전환 비용   | 쓰기 주도 경로     |
| **멀티 리더**      | 가용성↑             | 충돌/병합 복잡       | 지리 분산/쓰기 분산  |

## Apply to Our Domain (Orders/Dispatch)
- **홉별 타임아웃/재시도 예산**
  - GW **100ms** (retry 0) → OrderSvc **350ms** (내부 호출 2×**100ms**) → DB **250ms** (retry 0)
  - **Retry Budget ≤ 5%**, 지수 백오프(100→200→400ms)+지터
- **멱등/중복제거**
  - `POST /orders`: `Idempotency-Key` 저장(24h TTL) → 중복 200/동일 바디
  - 이벤트 소비자: **키-데두프 스토어**+**시퀀스 번호**(재정렬 허용 창)
- **FD & 리더십**
  - φ-accrual FD, 임계 φ=8에서 격리; 리더는 **리스(> clock_skew_p99 + RTT)** + **펜싱 토큰**
  - 배차 카운터 저장소: **N=3, W=2, R=2** (Raft/Quorum)
- **Tail 컷(선택적)**
  - `/orders/{id}` p95>300ms이고 에러버짓 여유>50%일 때 **헤지 1회**(중복 안전 경로만)

## Metrics & SLO (30일 롤링 권장)
- `dep.timeout_rate < 0.2%` (의존성별)
- `dep.retry_rate ≤ 5%`  *(budget)*
- `hedged_share ≤ 2%`    *(헤지 발사 비율)*
- `fd.false_suspect_rate < 0.1%/h`
- `clock_skew_p99 < 200ms`, `gc_pause_p99 < 50ms`
- `quorum_write_p95_ms < 120ms`
- `api.idempotency.dedup_hits` 추이(↑일수록 재시도 흡수)

## Open Questions (실험 계획으로 전환)
- 현재 RTT/큐잉 분해 기준으로 **홉별 타임아웃**은 적정한가?
- **헤지 조건**을 어느 엔드포인트에, 어떤 임계(p95, 에러버짓)로 둘까?
- 리더 **리스 기간**을 얼마로? (`lease >> clock_skew_p99 + RTT`)
- **Retry Budget** 소진 시 정책(스로틀/셰딩/에러)과 알람 기준은?
- 이벤트 스트림의 **재정렬 허용 창**(seconds)과 **데두프 TTL**은?

## Hands-on
### 실험 목표
- 재시도 정책이 장애 증폭을 일으키는지 검증하고 Retry Budget의 효과를 확인한다.

### 준비
- 의존 서비스에 인위적 지연/오류를 주입할 수 있는 테스트 환경
- 재시도 정책 토글(고정 간격 vs 백오프+지터+budget)

### 실행 단계
1. 의존 서비스에 5xx 20%, 지연 300ms를 10분간 주입한다.
2. 고정 간격 재시도로 요청 성공률/타임아웃/부하를 측정한다.
3. 백오프+지터+Retry Budget(5%)로 동일 테스트를 반복한다.
4. 선택 경로에서 조건부 헤지 1회를 적용해 p99 변화를 기록한다.

### 검증 메트릭
- `dep.retry_rate`
- `dep.timeout_rate`
- `upstream.qps`
- `api.latency_ms.p99`

### 실패 시 체크포인트
- retry_rate 급증 시 즉시 circuit open 및 재시도 차단
- 헤지 비율이 과도하면 발사 조건(p95 임계) 상향
